{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1nSB2J8YzwyrFqG4cW6uG10fupebf2P1P",
      "authorship_tag": "ABX9TyOghYMCHmYJENJBWmwKtzmx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumyamalviya92-pixel/prodigytask5/blob/main/task5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "id": "1wjX7rt3n6WU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = 512 if torch.cuda.is_available() else 256\n",
        "\n",
        "loader = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def load_image(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = loader(image).unsqueeze(0)\n",
        "    return image.to(device)\n",
        "\n",
        "content_img = load_image(\"content.jpg\")\n",
        "style_img = load_image(\"style.jpg\")\n",
        "\n",
        "assert content_img.size() == style_img.size()\n"
      ],
      "metadata": {
        "id": "hlPpWSHIr3n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(tensor, title=None):\n",
        "    image = tensor.cpu().clone()\n",
        "    image = image.squeeze(0)\n",
        "    image = transforms.ToPILImage()(image)\n",
        "    plt.imshow(image)\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "imshow(content_img, \"Content Image\")\n",
        "plt.subplot(1,2,2)\n",
        "imshow(style_img, \"Style Image\")\n"
      ],
      "metadata": {
        "id": "Aef_9MuqsZbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = models.vgg19(pretrained=True).features.to(device).eval()\n"
      ],
      "metadata": {
        "id": "N-ZfK8HFsmzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContentLoss(nn.Module):\n",
        "    def __init__(self, target):\n",
        "        super().__init__()\n",
        "        self.target = target.detach()\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.loss = nn.functional.mse_loss(x, self.target)\n",
        "        return x\n",
        "\n",
        "\n",
        "def gram_matrix(x):\n",
        "    b, c, h, w = x.size()\n",
        "    features = x.view(c, h * w)\n",
        "    gram = torch.mm(features, features.t())\n",
        "    return gram.div(c * h * w)\n",
        "\n",
        "\n",
        "class StyleLoss(nn.Module):\n",
        "    def __init__(self, target):\n",
        "        super().__init__()\n",
        "        self.target = gram_matrix(target).detach()\n",
        "\n",
        "    def forward(self, x):\n",
        "        G = gram_matrix(x)\n",
        "        self.loss = nn.functional.mse_loss(G, self.target)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "OH9VlmUjsq8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_layers = ['conv_4']\n",
        "style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
        "\n",
        "content_losses = []\n",
        "style_losses = []\n",
        "\n",
        "model = nn.Sequential()\n",
        "i = 0\n",
        "\n",
        "for layer in cnn.children():\n",
        "    if isinstance(layer, nn.Conv2d):\n",
        "        i += 1\n",
        "        name = f\"conv_{i}\"\n",
        "    elif isinstance(layer, nn.ReLU):\n",
        "        name = f\"relu_{i}\"\n",
        "        layer = nn.ReLU(inplace=False)\n",
        "    elif isinstance(layer, nn.MaxPool2d):\n",
        "        name = f\"pool_{i}\"\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "    model.add_module(name, layer)\n",
        "\n",
        "    if name in content_layers:\n",
        "        target = model(content_img).detach()\n",
        "        content_loss = ContentLoss(target)\n",
        "        model.add_module(\"content_loss_\" + name, content_loss)\n",
        "        content_losses.append(content_loss)\n",
        "\n",
        "    if name in style_layers:\n",
        "        target = model(style_img).detach()\n",
        "        style_loss = StyleLoss(target)\n",
        "        model.add_module(\"style_loss_\" + name, style_loss)\n",
        "        style_losses.append(style_loss)\n"
      ],
      "metadata": {
        "id": "2NJDS71vsy7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_img = content_img.clone()\n",
        "optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
        "\n",
        "style_weight = 1e6\n",
        "content_weight = 1\n",
        "\n",
        "epochs = 200\n",
        "print(\"Stylizing...\")\n",
        "\n",
        "run = [0]\n",
        "while run[0] <= epochs:\n",
        "\n",
        "    def closure():\n",
        "        optimizer.zero_grad()\n",
        "        model(input_img)\n",
        "        style_score = sum(sl.loss for sl in style_losses)\n",
        "        content_score = sum(cl.loss for cl in content_losses)\n",
        "\n",
        "        loss = style_weight * style_score + content_weight * content_score\n",
        "        loss.backward()\n",
        "\n",
        "        if run[0] % 50 == 0:\n",
        "            print(f\"Epoch {run[0]} | Style Loss: {style_score.item():.2f} | Content Loss: {content_score.item():.2f}\")\n",
        "\n",
        "        run[0] += 1\n",
        "        return loss\n",
        "\n",
        "    optimizer.step(closure)\n"
      ],
      "metadata": {
        "id": "xysKcSeks4M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "imshow(input_img, \"Stylized Image\")\n"
      ],
      "metadata": {
        "id": "qv0Wy10ztS-a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}